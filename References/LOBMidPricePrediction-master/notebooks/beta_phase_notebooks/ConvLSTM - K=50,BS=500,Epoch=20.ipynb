{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import zscore\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# batch size for model\n",
    "BATCH_SIZE = 500\n",
    "# number of epochs\n",
    "EPOCHS = 20\n",
    "# proportion of validation data\n",
    "VALIDATION_SPLIT = 0.2\n",
    "# learning rate and epsilon for ADAM optimizer\n",
    "LEARNING_RATE = 0.01\n",
    "EPSILON = 1\n",
    "# path where data is stored\n",
    "PATH=\"/Users/tanvipotdar/Projects/thesis/data/INTC_2015-01-01_2015-01-31_10\"\n",
    "# prediction horizon\n",
    "K = 50 \n",
    "# threshold to decide which category midprice direction falls in (up, down, stationary)\n",
    "ALPHA = 0.001\n",
    "\n",
    "# Static objects\n",
    "# instantiate one hot encoder here so that all classes will always map to the same labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    all_files = glob.glob(path + \"/*orderbook_10.csv\")\n",
    "    all_files.sort()\n",
    "    orderbooks = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=None)\n",
    "        orderbooks.append(df)\n",
    "    orderbook = pd.concat(orderbooks, axis=0, ignore_index=True)\n",
    "    col_names = ['ask_price_', 'ask_size_', 'bid_price_', 'bid_size_']\n",
    "    nums = map(str, range(1,11))\n",
    "    orderbook.columns = [y + x for x in nums for y in col_names]\n",
    "    return orderbook\n",
    "data = get_data(path=PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalise_data(data):\n",
    "    normalised_data = data.apply(zscore)\n",
    "    return normalised_data\n",
    "normalised_data = normalise_data(data)\n",
    "normalised_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# smoothed labelling of the midprice/ K is the prediction horizon\n",
    "def smooth_midprice_using_k_lookahead(normalised_data, k):\n",
    "    normalised_data['midprice'] = (normalised_data.ask_price_1+normalised_data.bid_price_1)/2\n",
    "    # mean of previous k mid-prices\n",
    "    normalised_data['m_minus'] = normalised_data['midprice'].rolling(window=k).mean()\n",
    "    # mean of next k mid-prices\n",
    "    normalised_data['m_plus'] = normalised_data['midprice'][::-1].rolling(window=k).mean()[::-1]\n",
    "    return normalised_data\n",
    "normalised_data = smooth_midprice_using_k_lookahead(normalised_data, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# label the smoothed mid-prices based on a threshold/ ALPHA is the threshold \n",
    "def create_midprice_labels(normalised_data):\n",
    "    normalised_data['change'] = (normalised_data.m_plus - normalised_data.m_minus)/normalised_data.m_minus\n",
    "    # assign categories up, down, stationary\n",
    "    normalised_data['label'] = pd.cut(normalised_data.change, bins=[-np.inf, -ALPHA, ALPHA, np.inf], \n",
    "                                    labels=['down', 'stationary', 'up'])\n",
    "    # drop all unlabelled values (will be first and last k values as they have no m_minus/m_plus value)\n",
    "    normalised_data.dropna(inplace=True)\n",
    "    return normalised_data\n",
    "normalised_data = create_midprice_labels(normalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "N = len(normalised_data) - len(normalised_data)%100\n",
    "def reshape_and_categorise_data(normalised_data, n):\n",
    "    data = normalised_data[:n]\n",
    "    cols = data.columns.to_list()[:40]\n",
    "    input_data = data[cols]\n",
    "    input_array = input_data.to_numpy().reshape(n//100,100,40,1)\n",
    "\n",
    "    output_data = data.label.to_numpy()[::-100][::-1]\n",
    "    integer_encoded = output_data.reshape(len(output_data), 1)\n",
    "    output_array = onehot_encoder.fit_transform(integer_encoded)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_array, output_array, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = reshape_and_categorise_data(normalised_data, N)\n",
    "print(\"Training input shape:\",X_train.shape)\n",
    "print(\"Test input shape:\", X_test.shape)\n",
    "print(\"Training output shape:\", y_train.shape)\n",
    "print(\"Test input shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# mapping of labels to one hot encoding\n",
    "encodings = [[1,0,0],[0,0,1],[0,1,0]]\n",
    "classes = onehot_encoder.inverse_transform(encodings)\n",
    "print(zip(classes, encodings))\n",
    "# Using np.argmax equates: 0-down, 1-stationary, 2-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # convolutional layers\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=16, kernel_size=(1,2), input_shape=(100,40,1), strides=(1, 2)))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.01))\n",
    "    model.add(keras.layers.Conv2D(filters=16, kernel_size=(4,1)))\n",
    "    model.add(keras.layers.Conv2D(filters=16, kernel_size=(4,1)))\n",
    "    model.add(keras.layers.Conv2D(filters=16, kernel_size=(1,2), strides=(1, 2)))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.01))\n",
    "    model.add(keras.layers.Conv2D(filters=16, kernel_size=(4,1)))\n",
    "    model.add(keras.layers.Conv2D(filters=16, kernel_size=(4,1)))\n",
    "    model.add(keras.layers.Conv2D(filters=16, kernel_size=(1,10), input_shape=(100,10,1)))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(4,1)))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(4,1)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(3,1)))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
    "\n",
    "    # lstm layer\n",
    "    model.add(keras.layers.LSTM(100))\n",
    "    model.add(keras.layers.Dense(3,activation='softmax'))\n",
    "    # compile model and summarize\n",
    "    adam = keras.optimizers.Adam(lr=LEARNING_RATE, epsilon=1)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT)\n",
    "    score, accuracy = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
    "    print(\"Accuracy is {}%\".format(accuracy*100))\n",
    "    return accuracy*100, history\n",
    "accuracy, history = fit_and_evaluate_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plots(history):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(history.history['loss'],'b--',lw=2,label='train_loss')\n",
    "    plt.plot(history.history['val_loss'],'g-',lw=2,label='val_loss')\n",
    "    plt.legend()\n",
    "    # plt.ylim([.5,1.3])\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(history.history['acc'],'b--',lw=2,label='train_acc')\n",
    "    plt.plot(history.history['val_acc'],'g-',lw=2,label='val_acc')\n",
    "    plt.legend()\n",
    "    # plt.ylim([.2,1.0])\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "plots(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_report(model, X_test, y_test, X_train, y_train):\n",
    "    target_names = ['down', 'stationary', 'up']\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    y_test_bool = np.argmax(y_test, axis=1)\n",
    "    print(classification_report(y_pred_bool, y_test_bool, target_names=target_names))\n",
    "    print(confusion_matrix(y_test_bool, y_pred_bool, labels=[0,1,2]))\n",
    "get_report(model, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}